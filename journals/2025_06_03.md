- {{video https://youtu.be/KHVR587oW8I?si=Ozn_en-aI3uO1Y97}}
	- {{youtube-timestamp 618}} [[surprise]]
		- $h(s) = \log{\frac{1}{p_s}}$
	- {{youtube-timestamp 736}} [[Entropy]]
		- average [[surprise]] of a distribution
		- $H = \sum_{s}{p_s \log{\frac{1}{p_s}}}$
		- {{youtube-timestamp 761}} higher: more uncertainty (distribution has greater variance)
	- {{youtube-timestamp 835}} [[cross entropy]]
		- $H(P,Q) = \sum_{s}{p_s \log{\frac{1}{q_s}}}$
		- {{youtube-timestamp 1017}} sources of surprise
		- {{youtube-timestamp 1061}} $$H(P,Q) \ge H(P)$$
		- {{youtube-timestamp 1079}} [[asymmetry]]
			- $H(P,Q) \ne H(Q,P)$
	- {{youtube-timestamp 1162}} [[Kullback-Leibler divergence]]
		- $D_{KL}(P,Q) = H(P,Q) - H(P)$
		- $D_{KL}(P,Q) = \sum_{s}{p_s \log{\frac{p_s}{q_s}}}$